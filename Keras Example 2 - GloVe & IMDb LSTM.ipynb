{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe Embeddings and IMDb Sentiment Analysis Example\n",
    "\n",
    "This notebook will have 2 parts: Part I will introduce the pretrained [GloVe](https://nlp.stanford.edu/projects/glove/) word embeddings by performing some fun word-arithmetic tasks. Part II will use the pretrained embeddings as an input and train a LSTM sentiment Analysis model on Keras's IMDb movie review dataset.\n",
    "\n",
    "The GloVe embeddings matrix is made open source by the Stanford NLP team. It was trained on the 2014 English language Wikipedia dump, the final model consists of 400,000 tokens (words), and is available in sizes of 50, 100, 200, and 300 dimensional vectors. This notebook gives us a brief overview of some of its capabilities.\n",
    "\n",
    "Credits to the Stanford NLP team: Jeffrey Pennington, Richard Socher, Christopher D. Manning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, operator\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Introduction to GloVe embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings:\n",
    "glove = {}\n",
    "with open('../GloVe Embeddings/glove.6B.100d.txt', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        glove[word] = coefs\n",
    "\n",
    "print('Found {} word vectors.'.format(len(glove)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.85189  ,  0.35649  ,  0.14484  , -0.61532  ,  0.61493  ,\n",
       "        0.2261   , -0.55096  ,  0.46157  , -0.019063 ,  0.56516  ,\n",
       "        0.17011  , -0.49439  , -0.18368  ,  0.08651  , -0.54403  ,\n",
       "        0.40244  ,  0.35977  ,  0.012714 , -0.23156  ,  0.081932 ,\n",
       "        0.031566 , -0.66883  , -0.18811  , -0.098277 , -0.2276   ,\n",
       "       -0.0044313,  0.14616  ,  0.069204 , -0.13451  ,  0.35255  ,\n",
       "       -0.24226  ,  0.21137  , -0.14358  ,  0.86754  , -0.83692  ,\n",
       "        0.045826 , -0.45233  , -0.32635  ,  0.57908  , -0.10124  ,\n",
       "        0.59631  ,  0.0056739, -0.57863  , -0.18945  ,  0.3612   ,\n",
       "        0.35982  , -0.58917  ,  0.028608 ,  0.46961  ,  0.32781  ,\n",
       "       -0.34656  , -0.33941  ,  0.10335  ,  0.31001  , -0.85238  ,\n",
       "       -0.77135  ,  0.38455  ,  0.56638  ,  0.3545   , -0.39816  ,\n",
       "       -0.91958  ,  0.17678  , -0.012436 , -0.28267  ,  0.52689  ,\n",
       "        0.42276  , -0.18496  , -0.28477  ,  0.20716  ,  0.44375  ,\n",
       "       -0.24254  , -0.03832  , -0.66316  ,  0.19424  ,  0.001448 ,\n",
       "       -0.8113   ,  0.78573  , -0.22446  , -0.41086  , -1.0168   ,\n",
       "        0.23853  ,  0.029208 ,  0.51246  ,  0.13137  , -0.071789 ,\n",
       "       -0.36634  , -0.5698   ,  0.20648  ,  0.1335   ,  0.17061  ,\n",
       "        0.47295  , -0.14812  ,  0.048966 , -0.24583  ,  0.1917   ,\n",
       "       -0.14573  ,  0.32606  ,  0.57959  ,  0.82071  , -0.23049  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A quick look at what word vectors look like:\n",
    "glove['nyc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8656660914421082\n"
     ]
    }
   ],
   "source": [
    "# A quick test for cosine similarity:\n",
    "m = glove['mother']\n",
    "f = glove['father']\n",
    "print(1 - cosine(m, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Let's have some fun with word arithmetics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analogy(word_a, word_b, word_c, embedding_matrix):\n",
    "    # Get the embedding vector for each input word:\n",
    "    e_a, e_b, e_c = embedding_matrix[word_a], embedding_matrix[word_b], embedding_matrix[word_c]\n",
    "\n",
    "    # Initialize max_cosine_sim as a large negative number and best_word as None:\n",
    "    words = embedding_matrix.keys()\n",
    "    max_cosine_sim = -100\n",
    "    best_word = None\n",
    "\n",
    "    # Loop over the whole word vector set:\n",
    "    for w in words:\n",
    "        # To avoid best_word being one of the input words, pass on them:\n",
    "        if w in (word_a, word_b, word_c):\n",
    "            continue\n",
    "\n",
    "        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c):\n",
    "        cosine_sim = 1 - cosine(e_b - e_a, embedding_matrix[w] - e_c)\n",
    "\n",
    "        # Keep track of best word:\n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "\n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokyo - Japan | Rome - \n",
      "answer: italy\n"
     ]
    }
   ],
   "source": [
    "# Test 1:\n",
    "rslt = word_analogy('tokyo', 'japan', 'rome', glove)\n",
    "print('Tokyo - Japan | Rome -', '\\nanswer:', rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man - woman | boy - \n",
      "answer: girl\n"
     ]
    }
   ],
   "source": [
    "# Test 2:\n",
    "rslt = word_analogy('man', 'woman', 'boy', glove)\n",
    "print('man - woman | boy -', '\\nanswer:', rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sky - blue | zebra - \n",
      "answer: striped\n"
     ]
    }
   ],
   "source": [
    "# Test 3 - Impressive:\n",
    "rslt = word_analogy('sky', 'blue', 'zebra', glove)\n",
    "print('sky - blue | zebra -', '\\nanswer:', rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spain - Madrid | Canada - \n",
      "answer: toronto\n"
     ]
    }
   ],
   "source": [
    "# Test 4 - Doesn't always work:\n",
    "rslt = word_analogy('spain', 'madrid', 'canada', glove)\n",
    "print('Spain - Madrid | Canada -', '\\nanswer:', rslt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Some more fun with closest neighbors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_neighbors(word, n, embedding_matrix):\n",
    "    # First, obtain the embedding vector for the input word:\n",
    "    word_vec = embedding_matrix[word]\n",
    "\n",
    "    # Compute cosine similarities across entire vocabulary set:\n",
    "    words = embedding_matrix.keys()\n",
    "    cosine_sims = {w:1 - cosine(word_vec, embedding_matrix[w]) for w in words}\n",
    "\n",
    "    # Obtain the n best matches (excluding the input word itself):\n",
    "    cosine_sims_sorted = sorted(cosine_sims.items(), key=operator.itemgetter(1))\n",
    "    synonyms_list = [i[0] for i in cosine_sims_sorted[-(n+1):-1]]\n",
    "    return synonyms_list[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yellow', 'blue', 'green', 'black', 'white']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 1:\n",
    "closest_neighbors('red', 5, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['courageous', 'fearless', 'proud', 'heroic', 'valiant']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 2:\n",
    "closest_neighbors('brave', 5, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fbi', 'intelligence', 'secret', 'covert', 'pentagon']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 3:\n",
    "closest_neighbors('cia', 5, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['herbivore', 'carnivorous', 'spamming', 'marsupial', 'tyrannosaurus']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 4 (what happened here...?):\n",
    "closest_neighbors('carnivore', 5, glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Sentiment analysis model using the IMDb movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, CuDNNLSTM, Bidirectional, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load IMDb dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDb dataset and limit to the top 5000 most frequent words:\n",
    "top_words = 5000\n",
    "word_idx_start = 3\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words, index_from=word_idx_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word-index dictionary so that we can view the dataset. Notice that for this dataset, we need to manually start the starting\n",
    "# indices to <PDA>, <START>, and <UNK>. These are the special start, unknown word, and zero-padding tokens:\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v + word_idx_start) for k, v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key, value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-star review:\n",
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly <UNK> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was <UNK> with us all\n",
      "\n",
      "1-star review:\n",
      "<START> a total waste of time just throw in a few explosions non stop fighting exotic cars a <UNK> <UNK> slow motion computer generated car <UNK> and last but not least a hugh <UNK> like character with wall to wall hot <UNK> and mix in a <UNK> and you will have this sorry excuse for a movie i really got a laugh out of the dr evil like heavily <UNK> <UNK> the plot was somewhere between <UNK> and non existent how many <UNK> are willing to make a 25 million dollar bet on a car race answer 4 but didn't they become <UNK> through <UNK> responsibility this was written for <UNK> <UNK> it plays like a video game i did enjoy the <UNK> ii <UNK> in the desert though\n"
     ]
    }
   ],
   "source": [
    "# Have a look at some reviews:\n",
    "print('5-star review:')\n",
    "print(' '.join(id_to_word[id] for id in X_train[0]))\n",
    "\n",
    "print('\\n1-star review:')\n",
    "print(' '.join(id_to_word[id] for id in X_train[15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preprocessing and create embedding matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate and pad the input sequences, using pad_sequences(), so that they are all the same length for modeling. The model will learn that the\n",
    "# zero values carry no information. The sequences are not the same length in terms of content, but same length vectors are required in Keras:\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our pre-trained embedding matrix using the GloVe vectors we loaded earlier:\n",
    "embedding_dim = 100\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_to_id.items():\n",
    "    embedding_vector = glove.get(word)\n",
    "    # Words not found in pretrained embedding will be all-zeros.\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "glove_layer = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_review_length, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Build LSTM model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with Sequential() as before. The first layer will be the GloVe word embeddings we've just processed:\n",
    "model = Sequential()\n",
    "model.add(glove_layer)\n",
    "\n",
    "# First recurrent layer. LSTM(n) will add a single LSTM layer with n recurrent units. Since we'll be feeding the output of this layer into\n",
    "# a second LSTM layer, we must specify return_sequences = True:\n",
    "model.add(CuDNNLSTM(embedding_dim, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Second recurrent layer. Notice that we're no longer specifying return_sequences = True, since this is the final recurrent layer of the network:\n",
    "model.add(CuDNNLSTM(embedding_dim))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer, sigmoid activation since this is a binary-classification problem:\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 100)          8858700   \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 500, 100)          80800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 100)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, 100)               80800     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 9,020,401\n",
      "Trainable params: 161,701\n",
      "Non-trainable params: 8,858,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# View model architecture:\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 51s 3ms/step - loss: 0.5600 - acc: 0.7048 - val_loss: 0.4743 - val_acc: 0.7982\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 49s 2ms/step - loss: 0.4051 - acc: 0.8215 - val_loss: 0.3433 - val_acc: 0.8504\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 48s 2ms/step - loss: 0.3504 - acc: 0.8495 - val_loss: 0.3271 - val_acc: 0.8606\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 48s 2ms/step - loss: 0.3148 - acc: 0.8695 - val_loss: 0.2955 - val_acc: 0.8762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21b9fb13b00>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the loss function, optimizer, batch size, and number of epochs:\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=4, batch_size=32, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Save/Load model (so that we don't have to retrain)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model:\n",
    "model.save('IMDb_LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model:\n",
    "model = load_model('IMDb_LSTM.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluate performance on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data predictions:\n",
    "test_probabilities = model.predict(X_test)\n",
    "test_probabilities = test_probabilities.reshape(len(test_probabilities),)\n",
    "test_predictions = np.array([int(round(i)) for i in test_probabilities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 23s 916us/step\n",
      "Test accuracy: 0.87656\n"
     ]
    }
   ],
   "source": [
    "# Accuracy evaluation:\n",
    "test_evaluation = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test accuracy:', test_evaluation[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.87      0.88     12500\n",
      "          1       0.87      0.88      0.88     12500\n",
      "\n",
      "avg / total       0.88      0.88      0.88     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Precision and recall stats:\n",
    "print(classification_report(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **View some predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment result function:\n",
    "def sentiment(n):\n",
    "    if n <= 0.5:\n",
    "        return 'Neg'\n",
    "    else:\n",
    "        return 'Pos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: Neg; Predicted: Neg; Pred. Probability: 0.026494808495044708\n",
      "--------------------------------------------------------------------\n",
      "<START> ok i admit that i still <UNK> <UNK> with la that was the main reason i went to see this film but it was so boring that i nearly felt asleep sorry but her talents as actress are not very convincing furthermore this film was presented as having outstanding special effects and cgi yeah for a b movie it is not that bad after having seen her in <UNK> some years ago also a very crappy film i thought that she would play more convincingly but la and may be the james bond the world is not enough seem to be the only good films with her is it her talent does she have a bad taste when <UNK> her films or simply bad luck\n"
     ]
    }
   ],
   "source": [
    "# View a random selection:\n",
    "rn = random.randint(0, len(test_predictions))\n",
    "print('Actual: {}; Predicted: {}; Pred. Probability: {}'.format(sentiment(y_test[rn]), sentiment(test_probabilities[rn]), test_probabilities[rn]))\n",
    "print('--------------------------------------------------------------------')\n",
    "print(' '.join(id_to_word[id] for id in X_test[rn] if id > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate to only the incorrect predictions:\n",
    "incorrects = []\n",
    "ind = 0\n",
    "for i in range(len(test_predictions)):\n",
    "    if y_test[i] != test_predictions[i]:\n",
    "        incorrects.append(ind)\n",
    "    ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: Neg; Predicted: Pos; Pred. Probability: 0.9176121950149536\n",
      "--------------------------------------------------------------------\n",
      "<START> <UNK> a previous summary says if you like aliens and <UNK> you will enjoy this film i could not disagree more this film pays no respect to its <UNK> <UNK> and has reduced two of the best loved sci fi <UNK> to little more than a teen horror slasher movie it has none of the tension or <UNK> present in previous alien or <UNK> movies and there is no <UNK> lead character i really did not care about any of the characters and i <UNK> <UNK> to see the stereotypical cast die as soon as possible in the <UNK> hope something better would <UNK> them it really takes super human <UNK> to have two of the most <UNK> creatures ever <UNK> <UNK> fail to make a gripping thrilling movie only watch this if you want to see how not to do it\n"
     ]
    }
   ],
   "source": [
    "rn = random.choice(incorrects)\n",
    "print('Actual: {}; Predicted: {}; Pred. Probability: {}'.format(sentiment(y_test[rn]), sentiment(test_probabilities[rn]), test_probabilities[rn]))\n",
    "print('--------------------------------------------------------------------')\n",
    "print(' '.join(id_to_word[id] for id in X_test[rn] if id > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
