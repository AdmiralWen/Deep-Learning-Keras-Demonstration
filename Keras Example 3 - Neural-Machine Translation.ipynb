{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English to French Neural-Machine Translation Example\n",
    "A working example of a sequence-to-sequence model using bidirectional LSTM layers.\n",
    "Training the model took ~2 hours on a machine with a quad-core 4200Mhz CPU, a GTX 1080Ti, and 64gb of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, string\n",
    "import numpy as np\n",
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and normalize raw text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to help clean and normalize the inputs:\n",
    "\n",
    "def to_pairs(text):\n",
    "    ''' Converts lines read from free text to English-French pairs. '''\n",
    "    lines = text.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs\n",
    "\n",
    "def _normalize_sentence(sent):\n",
    "    ''' Helper function for clean_pairs(); normalize the input sentences (remove puncuations, special characters, capitalizations, etc.). '''\n",
    "    re_print = re.compile(r'[^{}]'.format(re.escape(string.printable)))\n",
    "    punc_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    # Text normalization:\n",
    "    sent = normalize('NFD', sent).encode('ascii', 'ignore').decode('utf-8')  # Normalize unicode characters\n",
    "    sent = sent.split()                                                      # Tokenize on white space\n",
    "    sent = [word.lower() for word in sent]                                   # Convert to lower case\n",
    "    sent = [word.translate(punc_table) for word in sent]                     # Remove punctuation characters\n",
    "    sent = [re_print.sub('', w) for w in sent]                               # Remove non-printable characters\n",
    "    sent = [word for word in sent if word.isalpha()]                         # Remove non-word tokens\n",
    "\n",
    "    # Reformat as a string and return:\n",
    "    return ' '.join(sent)\n",
    "\n",
    "def clean_pairs(lines):\n",
    "    ''' Applies normalization (defined above) to each sentence pair in the input data. '''\n",
    "    cleaned = []\n",
    "    for pair in lines:\n",
    "        clean_pair = [_normalize_sentence(line) for line in pair]\n",
    "        cleaned.append(clean_pair)\n",
    "    return np.array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data and clean input sentences:\n",
    "with open('English-French.txt', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "sent_pairs = to_pairs(text)\n",
    "sent_pairs_cleaned = clean_pairs(sent_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['staying home isnt fun', 'rester chez soi na rien damusant'],\n",
       "       ['staying home isnt fun', 'rester chez soi nest pas marrant'],\n",
       "       ['stop biting your nails', 'arrete de ronger tes ongles'],\n",
       "       ['stop deluding yourself', 'arrete de te mentir a toimeme'],\n",
       "       ['stop deluding yourself', 'arretez de vous mentir a vousmeme']],\n",
       "      dtype='<U314')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at our cleaned input data:\n",
    "sent_pairs_cleaned[45000:45005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit our training data, and create train-test splits\n",
    "Some of the sentences from the dataset can be extremely long. The performance of LSTMs start to drop off if the sequences get too long (look into attention models for a potential enhancement). We also don't want the training time to take too long. So for this example, we'll limit the data to only the sentences with 5 words or fewer for English and 7 words or fewer for French. This results in about 67K training examples; we'll then split this into the first 55000 sentences for training, and the remaining for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset: 67222\n",
      "Training sentences: 55000 \n",
      "Testing sentences: 12222\n"
     ]
    }
   ],
   "source": [
    "# Limit to only the sentences with reasonable lengths (5-word English and 7-word French):\n",
    "sent_pairs_cleaned_sub = np.array([i for i in sent_pairs_cleaned if len(i[0].split()) <= 5 and len(i[1].split()) <= 7])\n",
    "print('Full dataset:', len(sent_pairs_cleaned_sub))\n",
    "\n",
    "# Use this for creating the tokenizers so that the sentence ordering stays the same:\n",
    "sent_pairs_cleaned_ordered = sent_pairs_cleaned_sub.copy()\n",
    "\n",
    "# Shuffle input data and split into training & testing:\n",
    "np.random.shuffle(sent_pairs_cleaned_sub)\n",
    "train = sent_pairs_cleaned_sub[:55000]\n",
    "test = sent_pairs_cleaned_sub[55000:]\n",
    "print('Training sentences:', len(train), '\\nTesting sentences:', len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and sequence encoding\n",
    "With our training and testing data created, the next step is to prepare the data for modeling. Namely:\n",
    "1. Tokenization: Separate sentences into arrays of individual words.\n",
    "2. Indexing: Map each word (for both languages) to an integer index. This is what will feed into the LSTM.\n",
    "3. Padding: Zero-pad sequences so that they're all the same length.\n",
    "4. Encode target: Since the target of the prediction will be a sequence as well, we must one-hot-encode the target sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector, TimeDistributed, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tokenizers:\n",
    "\n",
    "def create_tokenizer(lines):\n",
    "    ''' Keras tokenizer to be applied to each set of sentences. '''\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def max_length(lines):\n",
    "    ''' Function to compute the maximum sentence length for each language. '''\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocabulary size: 8345\n",
      "English max sentence length: 5\n",
      "French vocabulary size: 16001\n",
      "French max sentence length: 7\n"
     ]
    }
   ],
   "source": [
    "# English tokenizer:\n",
    "eng_tokenizer = create_tokenizer(sent_pairs_cleaned_ordered[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(sent_pairs_cleaned_ordered[:, 0])\n",
    "print('English vocabulary size:', eng_vocab_size)\n",
    "print('English max sentence length:', eng_length)\n",
    "\n",
    "# French tokenizer:\n",
    "fra_tokenizer = create_tokenizer(sent_pairs_cleaned_ordered[:, 1])\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "fra_length = max_length(sent_pairs_cleaned_ordered[:, 1])\n",
    "print('French vocabulary size:', fra_vocab_size)\n",
    "print('French max sentence length:', fra_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence encodings for both the source and target:\n",
    "\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    ''' Encodes text to interger-coded sequences, zero-padded based on max-length. '''\n",
    "    seq = tokenizer.texts_to_sequences(lines)\n",
    "    return pad_sequences(seq, maxlen=length, padding='post')\n",
    "\n",
    "def ohe_output(sequences, vocab_size):\n",
    "    ''' One-hot-encodes the target sentence. '''\n",
    "    ylist = [to_categorical(seq, num_classes=vocab_size) for seq in sequences]\n",
    "    y = np.array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data:\n",
    "trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_sequences(fra_tokenizer, fra_length, train[:, 1])\n",
    "trainY = ohe_output(trainY, fra_vocab_size)\n",
    "\n",
    "# Prepare validation data:\n",
    "testX = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_sequences(fra_tokenizer, fra_length, test[:, 1])\n",
    "testY = ohe_output(testY, fra_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build neural-machine translation model\n",
    "Here I chose to build a bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sequence-to-sequence LSTM model:\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "\n",
    "# As usual, we start with our embedding layer:\n",
    "model.add(Embedding(eng_vocab_size, embedding_dim, input_length=eng_length, mask_zero=True))\n",
    "\n",
    "# Encoding LSTM layer; notice that we pass the output to a RepeatVector() instead of using return_sequences because the output\n",
    "# sequence must feed into the decoding layer which has a different number of time steps:\n",
    "model.add(Bidirectional(LSTM(embedding_dim)))\n",
    "model.add(RepeatVector(fra_length))\n",
    "\n",
    "# Decoding LSTM layer; notice how we add a time-distributed dense layer at the end:\n",
    "model.add(Bidirectional(LSTM(embedding_dim, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(fra_vocab_size, activation='softmax')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 5, 100)            834500    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               160800    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 7, 200)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 7, 200)            240800    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 7, 16001)          3216201   \n",
      "=================================================================\n",
      "Total params: 4,452,301\n",
      "Trainable params: 4,452,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# View model architecture:\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 12222 samples\n",
      "Epoch 1/50\n",
      "55000/55000 [==============================] - 78s 1ms/step - loss: 4.6913 - val_loss: 4.2844\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.28439, saving model to Eng-Fra_Translation_2.h5\n",
      "Epoch 2/50\n",
      "55000/55000 [==============================] - 72s 1ms/step - loss: 3.9541 - val_loss: 3.7666\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.28439 to 3.76657, saving model to Eng-Fra_Translation_2.h5\n",
      "Epoch 3/50\n",
      "55000/55000 [==============================] - 71s 1ms/step - loss: 3.4187 - val_loss: 3.3790\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.76657 to 3.37896, saving model to Eng-Fra_Translation_2.h5\n",
      "Epoch 4/50\n",
      "55000/55000 [==============================] - 71s 1ms/step - loss: 3.0377 - val_loss: 3.1166\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.37896 to 3.11658, saving model to Eng-Fra_Translation_2.h5\n",
      "Epoch 5/50\n",
      "55000/55000 [==============================] - 70s 1ms/step - loss: 2.7230 - val_loss: 2.9091\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.11658 to 2.90914, saving model to Eng-Fra_Translation_2.h5\n",
      "Epoch 6/50\n",
      "55000/55000 [==============================] - 71s 1ms/step - loss: 2.4480 - val_loss: 2.7401\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.90914 to 2.74014, saving model to Eng-Fra_Translation_2.h5\n",
      "Epoch 7/50\n",
      "55000/55000 [==============================] - 71s 1ms/step - loss: 2.2035 - val_loss: 2.6059\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.74014 to 2.60592, saving model to Eng-Fra_Translation_2.h5\n",
      "Epoch 8/50\n",
      "55000/55000 [==============================] - 71s 1ms/step - loss: 1.9835 - val_loss: 2.4895\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.60592 to 2.48950, saving model to Eng-Fra_Translation_2.h5\n",
      "Epoch 9/50\n",
      "55000/55000 [==============================] - 71s 1ms/step - loss: 1.7932 - val_loss: 2.4033\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.48950 to 2.40335, saving model to Eng-Fra_Translation_2.h5\n",
      "Epoch 10/50\n",
      "55000/55000 [==============================] - 71s 1ms/step - loss: 1.6289 - val_loss: 2.3296\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.40335 to 2.32957, saving model to Eng-Fra_Translation_2.h5\n",
      "Epoch 11/50\n",
      "55000/55000 [==============================] - 71s 1ms/step - loss: 1.4891 - val_loss: 2.2791\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.32957 to 2.27915, saving model to Eng-Fra_Translation_2.h5\n",
      "Epoch 12/50\n",
      "55000/55000 [==============================] - 70s 1ms/step - loss: 1.3719 - val_loss: 2.2385\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.27915 to 2.23854, saving model to Eng-Fra_Translation_2.h5\n",
      "Epoch 13/50\n",
      "55000/55000 [==============================] - 71s 1ms/step - loss: 1.2725 - val_loss: 2.2078\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.23854 to 2.20782, saving model to Eng-Fra_Translation_2.h5\n",
      "Epoch 14/50\n",
      "55000/55000 [==============================] - 72s 1ms/step - loss: 1.1880 - val_loss: 2.1839\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.20782 to 2.18394, saving model to Eng-Fra_Translation_2.h5\n",
      "Epoch 15/50\n",
      "55000/55000 [==============================] - 71s 1ms/step - loss: 1.1151 - val_loss: 2.1662\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.18394 to 2.16623, saving model to Eng-Fra_Translation_2.h5\n",
      "Epoch 16/50\n",
      "55000/55000 [==============================] - 71s 1ms/step - loss: 1.0508 - val_loss: 2.1582\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.16623 to 2.15816, saving model to Eng-Fra_Translation_2.h5\n",
      "Epoch 17/50\n",
      "55000/55000 [==============================] - 69s 1ms/step - loss: 0.9925 - val_loss: 2.1449\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.15816 to 2.14494, saving model to Eng-Fra_Translation_2.h5\n",
      "Epoch 18/50\n",
      "55000/55000 [==============================] - 69s 1ms/step - loss: 0.9410 - val_loss: 2.1417\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.14494 to 2.14168, saving model to Eng-Fra_Translation_2.h5\n",
      "Epoch 19/50\n",
      "55000/55000 [==============================] - 69s 1ms/step - loss: 0.8961 - val_loss: 2.1430\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.14168\n",
      "Epoch 20/50\n",
      "55000/55000 [==============================] - 69s 1ms/step - loss: 0.8528 - val_loss: 2.1384\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.14168 to 2.13840, saving model to Eng-Fra_Translation_2.h5\n",
      "Epoch 21/50\n",
      "55000/55000 [==============================] - 69s 1ms/step - loss: 0.8136 - val_loss: 2.1431\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.13840\n",
      "Epoch 22/50\n",
      "55000/55000 [==============================] - 69s 1ms/step - loss: 0.7795 - val_loss: 2.1500\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.13840\n",
      "Epoch 23/50\n",
      "55000/55000 [==============================] - 69s 1ms/step - loss: 0.7469 - val_loss: 2.1566\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.13840\n",
      "Epoch 00023: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20b729809b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model; notice that we're defining some callback features to control the training process:\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "checkpoint = ModelCheckpoint('Eng-Fra_Translation_2.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "callbacks_list = [earlystop, checkpoint]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=50, batch_size=64, validation_data=(testX, testY), callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model:\n",
    "model.save('Eng-Fra_Translation_Test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model:\n",
    "model = load_model('Eng-Fra_Translation_BidirLSTM.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a look at some model translation outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table of id-to-word mappings:\n",
    "eng_id_to_word = {v:k for k, v in eng_tokenizer.word_index.items()}\n",
    "fra_id_to_word = {v:k for k, v in fra_tokenizer.word_index.items()}\n",
    "eng_id_to_word[0] = 0\n",
    "fra_id_to_word[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to help with predicting new sentences:\n",
    "\n",
    "def encode_input(sent, tokenizer, input_length):\n",
    "    ''' Encode an English input sentence according to our normalization and tokenization rules defined earlier. '''\n",
    "    sent_n = _normalize_sentence(sent)\n",
    "    return encode_sequences(tokenizer, input_length, [sent_n])\n",
    "\n",
    "def decode_prediction(pred, pred_length, id_to_word_mapping):\n",
    "    ''' Converts a prediction output matrix to readable French.\n",
    "        Note: The id_to_word_mapping must be built using the same French tokenizer as was used to train the model!\n",
    "    '''\n",
    "    pred_ids = [pred[0][i].argmax() for i in range(pred_length)]\n",
    "    pred_words = [id_to_word_mapping[i] for i in pred_ids if i != 0]\n",
    "    if pred_words[-1] == pred_words[-2]:\n",
    "        pred_words = pred_words[:-1]\n",
    "    return ' '.join(pred_words)\n",
    "\n",
    "def translate_eng_fra(in_sentence, model):\n",
    "    ''' Main translation function. '''\n",
    "    input_enc = encode_input(in_sentence, eng_tokenizer, eng_length)\n",
    "    pred_out = model.predict(input_enc)\n",
    "    return decode_prediction(pred_out, fra_length, fra_id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English : I love the movie.\n",
      "Model   : jadore le film\n",
      "-------------\n",
      "English : The weather is beautiful.\n",
      "Model   : il est est belle\n",
      "-------------\n",
      "English : God bless you!\n",
      "Model   : que dieu vous benisse\n",
      "-------------\n",
      "English : What's your favorite book?\n",
      "Model   : quel est ton livre prefere\n",
      "-------------\n",
      "English : My mother is well.\n",
      "Model   : ma mere est bien\n"
     ]
    }
   ],
   "source": [
    "# Try your own sentences:\n",
    "sentence = \"I love the movie.\"\n",
    "print('English :', sentence)\n",
    "print('Model   :', translate_eng_fra(sentence, model))\n",
    "print('-------------')\n",
    "\n",
    "sentence = \"The weather is beautiful.\"\n",
    "print('English :', sentence)\n",
    "print('Model   :', translate_eng_fra(sentence, model))\n",
    "print('-------------')\n",
    "\n",
    "sentence = \"God bless you!\"\n",
    "print('English :', sentence)\n",
    "print('Model   :', translate_eng_fra(sentence, model))\n",
    "print('-------------')\n",
    "\n",
    "sentence = \"What's your favorite book?\"\n",
    "print('English :', sentence)\n",
    "print('Model   :', translate_eng_fra(sentence, model))\n",
    "print('-------------')\n",
    "\n",
    "sentence = \"My mother is well.\"\n",
    "print('English :', sentence)\n",
    "print('Model   :', translate_eng_fra(sentence, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English : its not easy raising children\n",
      "French  : ce nest pas facile delever des enfants\n",
      "---------\n",
      "Model   : ce nest pas facile delever des enfants\n"
     ]
    }
   ],
   "source": [
    "# Try some random sentences from the test set:\n",
    "rn = np.random.choice(range(len(test)))\n",
    "rn_eng = test[rn][0]\n",
    "rn_fra = test[rn][1]\n",
    "print('English :', rn_eng)\n",
    "print('French  :', rn_fra)\n",
    "print('---------')\n",
    "pred = translate_eng_fra(rn_eng, model)\n",
    "print('Model   :', pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
